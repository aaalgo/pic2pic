#!/usr/bin/env python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import sys
sys.path.append('build/lib.linux-x86_64-2.7')
sys.path.append('tensorflow-vgg')
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import time
import threading
import subprocess
import logging
from tqdm import tqdm
import numpy as np
import tensorflow as tf
import tensorflow.contrib.slim as slim
import cv2
import picpac
import colorize_nets
from vgg19_bgr255 import Vgg19

AB_BINS = 313

flags = tf.app.flags
FLAGS = flags.FLAGS
flags.DEFINE_string('db', 'enhance.train', '')
flags.DEFINE_float('learning_rate', 0.02/100, 'initial learning rate.')
flags.DEFINE_bool('decay', True, '')
flags.DEFINE_float('decay_rate', 0.9, '')
flags.DEFINE_float('decay_steps', 10000, '')
flags.DEFINE_string('model', 'model', '')
flags.DEFINE_string('log', 'log', '')
flags.DEFINE_string('resume', None, '')

flags.DEFINE_integer('batch', 4, '')
flags.DEFINE_integer('max_steps', 400000, '')
flags.DEFINE_integer('epoch_steps', 50, '')
flags.DEFINE_integer('ckpt_epochs', 20, '')
flags.DEFINE_integer('verbose', logging.INFO, '')
flags.DEFINE_integer('max_to_keep', 200, '')
flags.DEFINE_integer('blur', 7, '')

flags.DEFINE_integer('generator_downscale', 0, '')
flags.DEFINE_integer('generator_blocks', 4, '')
flags.DEFINE_integer('generator_upscale', 0, '')    # should be 2

flags.DEFINE_integer('generator_filters', 64, '')
flags.DEFINE_integer('discriminator_size', 32, '')
flags.DEFINE_float('perceptual_weight', 1e0, '')
flags.DEFINE_float('smoothness_weight', 2e5, '')
flags.DEFINE_float('adversary_weight', 5e2, '')

# generate a function that returns parametric ReLU
def p_relu (alpha=0.25):
    return lambda net: tf.nn.relu(net) - alpha * tf.nn.relu(-net)

# lo-res -> hi-res
def generator (net, filters, scope='G'):
    # with scope 'G', all variable will be of a name 'G/...'
    # this way we can collect all generator variables to be trained with get_collection 'G'
    with tf.variable_scope(scope):
        net = slim.conv2d(net, filters, 7, 1, activation_fn=p_relu())
        # downscale
        for _ in range(FLAGS.generator_downscale):
            net = slim.conv2d(net, filters, 4, 2, activation_fn=p_relu())
        # residual block
        for _ in range(FLAGS.generator_blocks):
            residual = slim.conv2d(net, filters, 3, 1, activation_fn=p_relu(alpha=0.1))
            net = residual + net
        # upscale
        for _ in range(FLAGS.generator_upscale):
            # I think subpixel convolution is just conv_transpose
            # http://www.wdong.org/equivalence-of-subpixel-convolution-and-transposed-convolution.html
            net = slim.conv2d_transpose(net, filters, 6, 2, activation_fn=p_relu())
        net = slim.conv2d(net, 3, 7, 1, activation_fn=None)
        net = tf.clip_by_value(net, 0, 255)
    return net

# returns two feature vectors, L and H
# L is low-level perceptual feature
# H is high-level discriminative feature, +1: real data; -1: generated data
def feature_extractor (bgr255, scope='D'):
    c = FLAGS.discriminator_size
    vgg = Vgg19()   # vgg net is not trainable
    vgg.build(bgr255)
    with tf.variable_scope(scope):
        disc1_1 = slim.conv2d(slim.batch_norm(vgg.conv1_2), c, 5, 2, activation_fn=p_relu())
        disc1_2 = slim.conv2d(disc1_1, c, 5, 2)     # 1/4
        disc2 = slim.conv2d(slim.batch_norm(vgg.conv2_2), 2*c, 5, 2, activation_fn=p_relu())    # 1/4
        disc3 = slim.conv2d(slim.batch_norm(vgg.conv3_2), 3*c, 3, 1, activation_fn=p_relu())    # 1/4
        net = tf.concat([disc1_2, disc2, disc3], axis=3)
        net = slim.conv2d(net, 4*c, 1, 1, activation_fn=p_relu())
        net = slim.conv2d(net, 3*c, 3, 2, activation_fn=p_relu())
        net = slim.conv2d(net, 2*c, 1, 1, activation_fn=p_relu())
        disc = slim.batch_norm(slim.conv2d(net, 1, 1, 1, activation_fn=None))
    return vgg.conv2_2, disc

def softminus (x):
    return x - tf.nn.softplus(x)

def BGR2RGB (x):
    chs = tf.unstack(x, axis=-1)
    return tf.stack([chs[2], chs[1], chs[0]], axis=-1)

def build_graph (optimizer, global_step):

    zoom = 2**(FLAGS.generator_upscale - FLAGS.generator_downscale)

    # Y: hi-res
    #    holding original samples at train time
    Y = tf.placeholder(tf.float32, shape=(None, None, None, 3), name="images")

    # X: low-res image,
    #    generated by down-sizing hires image at train time
    hi_size = tf.slice(tf.shape(Y), [1], [2])   # (H, W)
    lo_size = hi_size // zoom

    X = Y

    if zoom == 1:
        pass
    else:
        X = tf.image.resize_images(X, lo_size)

    if not FLAGS.blur is None:
        radius = FLAGS.blur * 3
        kernel = np.zeros((radius*2+1, radius*2+1), dtype=np.float32)
        kernel[radius, radius] = 1.0
        #####!!!
        kernel = cv2.blur(kernel, (FLAGS.blur, FLAGS.blur))
        eye = np.eye(3)

        filters = np.reshape(np.outer(kernel, eye), kernel.shape + eye.shape)
        X = tf.nn.conv2d(X, tf.constant(filters, dtype=tf.float32), [1,1,1,1], 'SAME')

    X = tf.identity(X, name='lo_res')


    G = tf.identity(generator(X, FLAGS.generator_filters), name='hi_res')

    tf.summary.image('low_res', BGR2RGB(X), max_outputs=3)
    tf.summary.image('hi_res', BGR2RGB(Y), max_outputs=3)
    tf.summary.image('predict', BGR2RGB(G), max_outputs=3)

    # X and G need to go through the same feature extracting network
    # so we concatenate them first and then split the results
    L, H = feature_extractor(tf.concat([Y, G], axis=0))

    L1, L2 = tf.split(L, 2)  # low-level feature,    2 is prediction
    H1, H2 = tf.split(H, 2)  # high-level feature,   2 is prediction

    loss_perceptual = tf.reduce_mean(tf.square(L1 - L2), name='pe')

    sub = tf.shape(G) - tf.constant([0, 1, 1, 0], dtype=tf.int32)
    G0 = tf.slice(G, [0, 0, 0, 0], sub)
    Gx = tf.slice(G, [0, 0, 1, 0], sub)
    Gy = tf.slice(G, [0, 1, 0, 0], sub)

    loss_smooth = tf.identity(tf.reduce_mean(
                    tf.pow(tf.square(G0 - Gx) + tf.square(G0 - Gy), 1.25)) / 529357.9139706489,
                    name = 'sm')

    D_real = tf.reduce_mean(tf.nn.softplus(H1), name='dr')
    D_fake = tf.reduce_mean(softminus(H2), name='df')

    loss_adversary = tf.identity(1.0 - D_fake, name='ad')


    loss_G = tf.identity(loss_perceptual * FLAGS.perceptual_weight + 
                         loss_smooth * FLAGS.smoothness_weight + 
                         loss_adversary * FLAGS.adversary_weight, name='lg')

    loss_D = tf.identity(D_fake - D_real, name='ld')

    phases = []
    var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "G")
    phases.append(('generate',
                  optimizer.minimize(loss_G, global_step=global_step, var_list=var_list),
                  [loss_G, loss_perceptual, loss_smooth, loss_adversary],
                  [Y, G]))

    var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "D")
    phases.append(('discriminate',
                  optimizer.minimize(loss_D, global_step=global_step, var_list=var_list),
                  [D_real, D_fake],
                  []))

    return Y, phases

def main (_):
    logging.basicConfig(level=FLAGS.verbose)
    try:
        os.makedirs(FLAGS.model)
    except:
        pass
    assert FLAGS.db and os.path.exists(FLAGS.db)

    rate = tf.constant(FLAGS.learning_rate)
    global_step = tf.Variable(0, name='global_step', trainable=False)
    if FLAGS.decay:
        rate = tf.train.exponential_decay(rate, global_step, FLAGS.decay_steps, FLAGS.decay_rate, staircase=True)
        tf.summary.scalar('learning_rate', rate)
    optimizer = tf.train.AdamOptimizer(rate)

    Y, phases = build_graph(optimizer, global_step)

    metric_names = []
    for _, _, metrics, _ in phases:
        metric_names.extend([x.name[:-2] for x in metrics])

    init = tf.global_variables_initializer()

    saver = tf.train.Saver(max_to_keep=FLAGS.max_to_keep)

    summaries = tf.summary.merge_all()
    if FLAGS.resume is None:
        if FLAGS.log[0] != '/':
            subprocess.check_call("rm -rf %s" % FLAGS.log, shell=True)
    log = tf.summary.FileWriter(FLAGS.log, tf.get_default_graph(), flush_secs=20)

    tf.get_default_graph().finalize()

    picpac_config = dict(seed=2016,
                cache=False,    # input 1280x718
                #max_size=200,
                #min_size=192,
                crop_width=560,
                crop_height=320,
                shuffle=True,
                reshuffle=True,
                batch=FLAGS.batch,
                round_div=2**FLAGS.generator_downscale,
                channels=3,
                stratify=False,
                pert_min_scale=0.45, # 576  x 323
                pert_max_scale=0.55,
                channel_first=False # this is tensorflow specific
                                    # Caffe's dimension order is different.
                )

    stream = picpac.ImageStream(FLAGS.db, perturb=True, loop=True, **picpac_config)

    sess_config = tf.ConfigProto()

    with tf.Session(config=sess_config) as sess:
        sess.run(init)
        if FLAGS.resume:
            saver.restore(sess, FLAGS.resume)
        step = 0
        epoch = 0
        global_start_time = time.time()
        while step < FLAGS.max_steps:
            start_time = time.time()
            avg = np.array([0] * len(metric_names), dtype=np.float32)
            for _ in tqdm(range(FLAGS.epoch_steps), leave=False):
                images, _, _ = stream.next()
                forward_dict = {}
                m_off = 0
                for _, train, metrics, forward in phases:
                    feed_dict = {Y: images}
                    feed_dict.update(forward_dict)
                    _, m, f = sess.run([train, metrics, forward], feed_dict=feed_dict)
                    forward_dict = dict(zip(forward, f))
                    m_off_n = m_off + len(metrics)
                    avg[m_off:m_off_n] += m
                    m_off = m_off_n
                    pass
                step += 1
                pass
            images, _, _ = stream.next()
            s, = sess.run([summaries], feed_dict={Y: images})
            log.add_summary(s, step)
            avg /= FLAGS.epoch_steps
            stop_time = time.time()
            epoch += 1
            saved = ''
            if epoch % FLAGS.ckpt_epochs == 0:
                saver.save(sess, '%s/%d' % (FLAGS.model, step))
                saved = ' saved'

            txt = ', '.join(['%s=%.4f' % (a, b) for a, b in zip(metric_names, list(avg))])
            print('step=%d elapsed=%.4f/%.4f  %s%s'
                    % (step, (stop_time - start_time), (stop_time - global_start_time), txt, saved))
            pass
        pass
        log.close()
    pass

if __name__ == '__main__':
    tf.app.run()

